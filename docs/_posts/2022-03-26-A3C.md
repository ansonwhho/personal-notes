---
layout: article
title: "Asynchronous Methods for Deep Reinforcement Learning (Mnih, 2016)"
sidebar:
  nav: docs-en
aside:
  toc: true
show_tags: true
tags: AI papers reinforcement-learning deep-learning
---

*[Link to the paper by Mnih et al.](https://arxiv.org/pdf/1602.01783.pdf)*

# Motivation
While experience replay is good for increasing stability in online RL[^1], it is computationally expensive and requires off-policy learning algorithms. What if we had another approach that maintains stability of learning, without these downsides? 

# Approach
## Asynchronous RL Framework
In response to these problems, the authors suggest a new paradigm, which applies two key ideas:
1. Use **asynchronous actor-learners**: have multiple actors running in parallel on multiple CPU threads[^2]
2. Multiple actor-learners in parallel are likely to explore different parts of the environment. Using different exploration policies in each actor helps maximise diversity

This has several benefits:
- Noise reduction: it decorrelates agent data such that it is more stationary (since parallel agents experience a wide range of states)
- It works with on-policy RL
- Improved computational efficiency
- Better performance across multiple tasks, reaching SoTA (at the time)
- Less training time (linear in the number of parallel actor-learners)

The authors apply this framework to four common RL algorithms:
1. One-step $SARSA$
2. One-step $Q$-learning
3. $n$-step $Q$-learning
4. Advantage actor-critic

## A3C Algorithm
We focus on the last one, because this was arguably the main contribution of the paper - the **Asynchronous advantage actor-critic (A3C)** algorithm. 

In comparison with a standard actor-critic algorithm, this has several differences:
- It focuses on the *advantage* function $A(s_t, a_t; \theta, \theta_v)$ rather than the state or action values
- It has multiple actors that learn policies $\pi$, rather than just a single one
- Updating these policies is done *asynchronously*, so updates are made without waiting for all actors to finish a particular segment of experience[^3]

Learning is done using policy gradient methods, with update equation

$$\theta \leftarrow \theta + \alpha \nabla_{\theta'} \log \pi(a_t \mid s_t; \theta') A(s_t, a_t; \theta, \theta_v).$$

Interestingly, the authors add the entropy of the policy to the objective function, which helped improve exploration by preventing premature convergence to suboptimal policies. 

# Evaluation
These algorithms were tested in four different domains, e.g. Atari games. They also evaluate performance in continuous action spaces - the A3C algorithm extends to these cases quite naturally, but value-based algorithms do not (methods like $Q$-learning are based on the discrete-space version of the Bellman equation).

They find that: 
- A3C significantly improves on the SoTA average score over 57 Atari games, often in half the training time
- A3C was able to find successful policies even in continuous action spaces (e.g. in the MuJoCo physics simulator)
- Asynchronous one-step $Q$-learning and $SARSA$ show superlinear speedups in training, that cannot be explained purely computational gains (the authors attribute this to multiple threads helping to reduce the bias in one-step methods)
- All methods are fairly robust to different learning rates and initialisations

These findings suggest that stable online $Q$-learning is possible without experience replay, but using it can improve data efficiency and training time. 


[^1]: This is done by solving two problems: (1) Temporally adjacent states are typically correlated, breaking the i.i.d. assumption in many forms of gradient descent. (2) Observations are typically quite noisy. Drawing batches from a replay buffer decorrelates the observations and makes learning less sensitive to noise.

[^2]: Running each actor on the same CPU removes communication costs across CPUs.

[^3]: Another approach is to do *synchronous* updates, which is the A2C algorithm. 